import torch
import torch.nn as nn
import math


class TransformerEncoderDecoderModel(nn.Module):
    def __init__(
        self, 
        input_dim, 
        d_model=256,
        nhead=8,
        num_encoder_layers=4,
        num_decoder_layers=2,
        output_dim=1, 
        pred_len=5, 
        dropout=0.2
    ):
        """
        Encoder-Decoder Transformer for Multi-Step Stock Price Forecasting
        
        Architecture:
        1. Encoder: è™•ç†æ­·å²åƒ¹æ ¼åºåˆ— (seq_len å¤©)
        2. Decoder: è‡ªå›æ­¸ç”Ÿæˆæœªä¾†é æ¸¬ (pred_len å¤©)
        3. Cross-Attention: Decoder å‹•æ…‹é—œæ³¨æ­·å²é—œéµæ™‚æ®µ
        
        Args:
            input_dim: è¼¸å…¥ç‰¹å¾µç¶­åº¦ (10å€‹ç‰¹å¾µ)
            d_model: Transformer å…§éƒ¨ç¶­åº¦ (256)
            nhead: Multi-head attention çš„é ­æ•¸ (8)
            num_encoder_layers: Encoder å±¤æ•¸ (4)
            num_decoder_layers: Decoder å±¤æ•¸ (2)
            output_dim: è¼¸å‡ºç¶­åº¦ (1 = è‚¡åƒ¹)
            pred_len: é æ¸¬å¤©æ•¸ (5)
            dropout: Dropout ç‡ (0.2)
        
        Example:
            >>> model = TransformerEncoderDecoderModel(
            ...     input_dim=10, d_model=256, nhead=8,
            ...     num_encoder_layers=4, num_decoder_layers=2,
            ...     pred_len=5, dropout=0.2
            ... )
            >>> x = torch.randn(32, 60, 10)  # [batch, seq_len, features]
            >>> output = model(x)  # [batch, 5]
        """
        super().__init__()
        self.pred_len = pred_len
        self.d_model = d_model
        self.input_dim = input_dim
        
        # ========== Input Projection Layer ==========
        # å°‡ input_dim (10) æŠ•å½±åˆ° d_model (256)
        self.input_projection = nn.Linear(input_dim, d_model)
        
        # ========== Positional Encoding ==========
        self.pos_encoder = PositionalEncoding(d_model, dropout=dropout)
        
        # ========== Transformer Encoder ==========
        # è™•ç†æ­·å²åºåˆ—ï¼Œæå–ç‰¹å¾µ
        encoder_layers = nn.TransformerEncoderLayer(
            d_model=d_model,
            nhead=nhead,
            dim_feedforward=d_model * 4,  # Feed-forward ç¶­åº¦
            dropout=dropout,
            batch_first=True,  # ä½¿ç”¨ [batch, seq, feature] æ ¼å¼
            norm_first=True    # Pre-Layer Normalization (æ›´ç©©å®š)
        )
        self.encoder = nn.TransformerEncoder(
            encoder_layers, 
            num_encoder_layers,
            norm=nn.LayerNorm(d_model)  # æœ€å¾Œçš„ Layer Norm
        )
        
        # ========== Transformer Decoder ==========
        # è‡ªå›æ­¸ç”Ÿæˆæœªä¾†é æ¸¬
        decoder_layers = nn.TransformerDecoderLayer(
            d_model=d_model,
            nhead=nhead,
            dim_feedforward=d_model * 4,
            dropout=dropout,
            batch_first=True,
            norm_first=True
        )
        self.decoder = nn.TransformerDecoder(
            decoder_layers,
            num_decoder_layers,
            norm=nn.LayerNorm(d_model)
        )
        
        # ========== Learnable Decoder Input (Query Embeddings) ==========
        # ä»£è¡¨ 5 å€‹é æ¸¬å¤©çš„ã€Œèµ·å§‹å‘é‡ã€
        # æ¨¡å‹æœƒå­¸ç¿’é€™äº›å‘é‡ï¼Œä½¿å…¶ä»£è¡¨ã€Œæ˜å¤©ã€ã€ã€Œå¾Œå¤©ã€...ã€Œ5å¤©å¾Œã€
        self.decoder_input = nn.Parameter(torch.randn(pred_len, d_model))
        self._init_decoder_input()
        
        # ========== Output Projection Layer ==========
        # å°‡ d_model (256) æŠ•å½±å›è‚¡åƒ¹ç©ºé–“ (1)
        self.fc_out = nn.Linear(d_model, output_dim)
        
        # ========== Weight Initialization ==========
        self._init_weights()
    
    def _init_decoder_input(self):
        """åˆå§‹åŒ– Decoder Input (Query Embeddings)"""
        nn.init.xavier_uniform_(self.decoder_input.unsqueeze(0))
    
    def _init_weights(self):
        """æ¬Šé‡åˆå§‹åŒ– (Xavier Uniform)"""
        for p in self.parameters():
            if p.dim() > 1:
                nn.init.xavier_uniform_(p)
    
    def forward(self, src):
        """
        Forward pass
        
        Args:
            src: [batch, seq_len, input_dim]
                 ä¾‹å¦‚ [32, 60, 10] - 32å€‹æ¨£æœ¬ï¼Œ60å¤©æ­·å²ï¼Œ10å€‹ç‰¹å¾µ
        
        Returns:
            predictions: [batch, pred_len]
                        ä¾‹å¦‚ [32, 5] - 32å€‹æ¨£æœ¬ï¼Œ5å¤©é æ¸¬
        
        Processing Flow:
        1. Input Projection: [batch, 60, 10] â†’ [batch, 60, 256]
        2. Positional Encoding: åŠ å…¥ä½ç½®ä¿¡æ¯
        3. Encoder: è™•ç†æ­·å² â†’ memory [batch, 60, 256]
        4. Decoder Input: æº–å‚™ 5 å€‹ queries [batch, 5, 256]
        5. Decoder: è‡ªå›æ­¸ç”Ÿæˆ â†’ [batch, 5, 256]
        6. Output Projection: [batch, 5, 256] â†’ [batch, 5, 1] â†’ [batch, 5]
        """
        batch_size = src.size(0)
        
        # ========== Step 1: Encoder - è™•ç†æ­·å²åºåˆ— ==========
        # 1.1 Input Projection
        src = self.input_projection(src)  # [batch, seq_len, d_model]
        
        # 1.2 ç¸®æ”¾ (Attention is All You Need è«–æ–‡å»ºè­°)
        src = src * math.sqrt(self.d_model)
        
        # 1.3 åŠ å…¥ä½ç½®ç·¨ç¢¼
        src = self.pos_encoder(src)
        
        # 1.4 Encoder è™•ç†
        memory = self.encoder(src)  # [batch, seq_len, d_model]
        # memory åŒ…å«äº†æ­·å²åºåˆ—çš„æ‰€æœ‰ä¿¡æ¯
        
        # ========== Step 2: Decoder - è‡ªå›æ­¸ç”Ÿæˆé æ¸¬ ==========
        # 2.1 æº–å‚™ Decoder Input (Query Embeddings)
        tgt = self.decoder_input.unsqueeze(0).expand(batch_size, -1, -1)
        # tgt: [batch, pred_len, d_model]
        # ä¾‹å¦‚: [32, 5, 256] - 5 å€‹ learnable queries
        
        # 2.2 åŠ å…¥ä½ç½®ç·¨ç¢¼ (è®“æ¨¡å‹çŸ¥é“ Day 1, Day 2, ..., Day 5)
        tgt = self.pos_encoder(tgt)
        
        # 2.3 ç”Ÿæˆ Causal Mask
        # ç¢ºä¿ Day i åªèƒ½çœ‹åˆ° Day 1..i-1 çš„é æ¸¬
        causal_mask = self._generate_square_subsequent_mask(self.pred_len).to(src.device)
        
        # ğŸ” Debug: åˆ—å° mask (åªåœ¨ç¬¬ä¸€æ¬¡)
        if not hasattr(self, '_mask_printed'):
            print(f"\n{'='*60}")
            print(f"Decoder Causal Mask (size: {causal_mask.shape}):")
            print(causal_mask)
            print(f"{'='*60}\n")
            self._mask_printed = True
        
        # 2.4 Decoder è™•ç†
        output = self.decoder(
            tgt=tgt,              # [batch, 5, d_model] - è¦ç”Ÿæˆçš„å…§å®¹
            memory=memory,        # [batch, 60, d_model] - æ­·å²ä¿¡æ¯
            tgt_mask=causal_mask  # [5, 5] - Causal mask
        )
        # output: [batch, pred_len, d_model]
        
        # Decoder å…§éƒ¨ç™¼ç”Ÿçš„äº‹æƒ…:
        # - Self-Attention: Day 2 çœ‹åˆ° Day 1ï¼ŒDay 3 çœ‹åˆ° Day 1-2...
        # - Cross-Attention: æ¯ä¸€å¤©éƒ½å‹•æ…‹é—œæ³¨æ­·å²åºåˆ— (memory)
        # - Feed-Forward: éç·šæ€§è½‰æ›
        
        # ========== Step 3: Output Projection ==========
        # 3.1 æŠ•å½±åˆ°è‚¡åƒ¹ç©ºé–“
        predictions = self.fc_out(output)  # [batch, pred_len, 1]
        
        # 3.2 å»æ‰æœ€å¾Œä¸€ç¶­
        predictions = predictions.squeeze(-1)  # [batch, pred_len]
        
        return predictions
    
    def _generate_square_subsequent_mask(self, sz):
        """
        ç”Ÿæˆ Causal Mask (ä¸Šä¸‰è§’ç‚º -infï¼Œä¸‹ä¸‰è§’ç‚º 0)
        
        é€™å€‹ mask ç¢ºä¿è‡ªå›æ­¸ç‰¹æ€§ï¼š
        - Day 1 åªèƒ½çœ‹åˆ°è‡ªå·±
        - Day 2 å¯ä»¥çœ‹åˆ° Day 1-2
        - Day 5 å¯ä»¥çœ‹åˆ° Day 1-5
        
        Args:
            sz: åºåˆ—é•·åº¦ (pred_len = 5)
        
        Returns:
            mask: [sz, sz] çš„ä¸Šä¸‰è§’çŸ©é™£
        
        Example for sz=5:
            [[0,    -inf, -inf, -inf, -inf],
             [0,    0,    -inf, -inf, -inf],
             [0,    0,    0,    -inf, -inf],
             [0,    0,    0,    0,    -inf],
             [0,    0,    0,    0,    0   ]]
        
        åœ¨ Attention è¨ˆç®—ä¸­:
        - 0 çš„ä½ç½®: å¯ä»¥é—œæ³¨ (attention weight æ­£å¸¸è¨ˆç®—)
        - -inf çš„ä½ç½®: ä¸èƒ½é—œæ³¨ (attention weight = 0)
        """
        mask = torch.triu(torch.ones(sz, sz) * float('-inf'), diagonal=1)
        return mask


class PositionalEncoding(nn.Module):
    def __init__(self, d_model, dropout=0.1, max_len=5000):
        """
        Positional Encoding (Attention is All You Need è«–æ–‡)
        
        ç‚ºåºåˆ—ä¸­çš„æ¯å€‹ä½ç½®ç”Ÿæˆå”¯ä¸€çš„ç·¨ç¢¼ï¼Œè®“æ¨¡å‹çŸ¥é“ï¼š
        - å“ªå€‹æ˜¯ã€Œæ˜¨å¤©ã€ï¼Œå“ªå€‹æ˜¯ã€Œ60å¤©å‰ã€
        - å“ªå€‹æ˜¯ã€Œæ˜å¤©ã€ï¼Œå“ªå€‹æ˜¯ã€Œ5å¤©å¾Œã€
        
        ä½¿ç”¨ Sinusoidal Function:
        PE(pos, 2i)   = sin(pos / 10000^(2i/d_model))
        PE(pos, 2i+1) = cos(pos / 10000^(2i/d_model))
        
        å„ªé»:
        - å¯ä»¥è™•ç†ä»»æ„é•·åº¦çš„åºåˆ—
        - ç›¸å°ä½ç½®é—œä¿‚æ¸…æ™°
        - ä¸éœ€è¦è¨“ç·´
        
        Args:
            d_model: æ¨¡å‹ç¶­åº¦ (256)
            dropout: Dropout ç‡ (0.1)
            max_len: æœ€å¤§åºåˆ—é•·åº¦ (5000)
        """
        super().__init__()
        self.dropout = nn.Dropout(p=dropout)
        
        # å‰µå»ºä½ç½®ç·¨ç¢¼çŸ©é™£ [max_len, d_model]
        pe = torch.zeros(max_len, d_model)
        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)
        
        # è¨ˆç®— div_term: 10000^(2i/d_model)
        div_term = torch.exp(
            torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model)
        )
        
        # å¶æ•¸ç¶­åº¦ä½¿ç”¨ sin
        pe[:, 0::2] = torch.sin(position * div_term)
        # å¥‡æ•¸ç¶­åº¦ä½¿ç”¨ cos
        pe[:, 1::2] = torch.cos(position * div_term)
        
        # å¢åŠ  batch ç¶­åº¦ [1, max_len, d_model]
        pe = pe.unsqueeze(0)
        
        # è¨»å†Šç‚º buffer (ä¸æœƒè¢«ç•¶ä½œåƒæ•¸è¨“ç·´)
        self.register_buffer('pe', pe)
    
    def forward(self, x):
        """
        Args:
            x: [batch, seq_len, d_model]
        
        Returns:
            x + positional encoding: [batch, seq_len, d_model]
        """
        # åªå–éœ€è¦çš„é•·åº¦
        x = x + self.pe[:, :x.size(1), :]
        return self.dropout(x)